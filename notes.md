# Matrix Math

 - dot product - rows of first matrix, cols of second matrix. calc dot product (multiply each pair, then add them together). put the result in the row x col used to calc in resulting matrix.
 - requirement - number of cols in left = number of rows in right (inner dimensions are same)
 - resulting matrix - rows in right x cols in left (outer dimensions are result)
 - store records as rows, not columns
# Logistic Regression
 - logloss function
 - minimize by gradient decent
 
# Perceptron
 - activation function - given inputs, what should be the output?
 - bias - shift line (constant)
 - sigmoid = 1/(1+e^-x)
 - activation functions, if they are continuous and differentiable, then you can do gradient decent
 - sse - sum of squared errors
 
